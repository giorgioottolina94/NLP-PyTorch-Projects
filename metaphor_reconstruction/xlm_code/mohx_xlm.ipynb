{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"},"colab":{"name":"mohx_xlm.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"QbOvSBgh_K2T","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624829958695,"user_tz":-120,"elapsed":467,"user":{"displayName":"Giorgio Ottolina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiHqc78Tp0LGpgqAFdZiknA8pFcmI4JwnryQM6I=s64","userId":"08747627632687137907"}},"outputId":"bcb6bbf1-1af2-42ab-ad85-21f97a8790a8"},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vzqIeh0k_MbM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624829961737,"user_tz":-120,"elapsed":2519,"user":{"displayName":"Giorgio Ottolina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiHqc78Tp0LGpgqAFdZiknA8pFcmI4JwnryQM6I=s64","userId":"08747627632687137907"}},"outputId":"952be927-76c9-43cf-a5ff-3ffb4123ca58"},"source":["import tensorflow as tf\n","\n","# Get the GPU device name.\n","device_name = tf.test.gpu_device_name()\n","\n","# The device name should look like the following:\n","if device_name == '/device:GPU:0':\n","    print('Found GPU at: {}'.format(device_name))\n","else:\n","    raise SystemError('GPU device not found')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found GPU at: /device:GPU:0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1Og8o_HG_Mf4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624829962212,"user_tz":-120,"elapsed":480,"user":{"displayName":"Giorgio Ottolina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiHqc78Tp0LGpgqAFdZiknA8pFcmI4JwnryQM6I=s64","userId":"08747627632687137907"}},"outputId":"ce568d90-0052-476c-a5d1-a4b92302ed59"},"source":["import torch\n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla V100-SXM2-16GB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8uCANj-7fD_L"},"source":["import logging\n","logging.basicConfig(level=logging.ERROR)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JI3V3_oy_Mlp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624829964750,"user_tz":-120,"elapsed":2545,"user":{"displayName":"Giorgio Ottolina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiHqc78Tp0LGpgqAFdZiknA8pFcmI4JwnryQM6I=s64","userId":"08747627632687137907"}},"outputId":"b4b5c1c1-2173-481f-86c8-26cb22326379"},"source":["!pip install transformers==3"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers==3 in /usr/local/lib/python3.7/dist-packages (3.0.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3) (2019.12.20)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3) (1.19.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3) (4.41.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3) (20.9)\n","Requirement already satisfied: tokenizers==0.8.0-rc4 in /usr/local/lib/python3.7/dist-packages (from transformers==3) (0.8.0rc4)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from transformers==3) (0.1.96)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3) (0.0.45)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3) (3.0.12)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3) (3.0.4)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3) (2.4.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3) (1.15.0)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MspPBjFecRHv"},"source":["#Data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7gn-qmxXFkvG","executionInfo":{"status":"ok","timestamp":1624829964751,"user_tz":-120,"elapsed":34,"user":{"displayName":"Giorgio Ottolina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiHqc78Tp0LGpgqAFdZiknA8pFcmI4JwnryQM6I=s64","userId":"08747627632687137907"}},"outputId":"41ee3dcd-598c-40d5-b1f6-c846b12f57de"},"source":["cd drive/My Drive/Colab Notebooks/experiments"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks/experiments\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ekbV40xzFsDB"},"source":["# Use a subset for quick experiments\n","#data = data[:10000]\n","\n","from sklearn.model_selection import train_test_split as tts\n","import pandas as pd\n","data = pd.read_csv(\"data/moh-x.csv\")\n","\n","# Split to train, val and test\n","train, test_df = tts(data[[\"sentence\", \"arg1\", \"verb\", \"label\"]], random_state=42, test_size=0.1)\n","train, val = tts(train, random_state=42, test_size=test_df.shape[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sT3SnDiB_MoJ","colab":{"base_uri":"https://localhost:8080/","height":242},"executionInfo":{"status":"ok","timestamp":1624829964758,"user_tz":-120,"elapsed":28,"user":{"displayName":"Giorgio Ottolina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiHqc78Tp0LGpgqAFdZiknA8pFcmI4JwnryQM6I=s64","userId":"08747627632687137907"}},"outputId":"0253982a-2e57-4fe3-f09e-1af774b99c90"},"source":["import pandas as pd\n","# import pytreebank\n","\n","#cd drive/My Drive/Colab Notebooks/experiments/data\n","\n","# Load the dataset into a pandas dataframe.\n","df = pd.read_csv(\"data/moh-x.csv\")\n","# Report the number of sentences.\n","print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n","df.head()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of training sentences: 647\n","\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>arg1</th>\n","      <th>arg2</th>\n","      <th>verb</th>\n","      <th>sentence</th>\n","      <th>verb_idx</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>knowledge</td>\n","      <td>NaN</td>\n","      <td>absorb</td>\n","      <td>He absorbed the knowledge or beliefs of his t...</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>cost</td>\n","      <td>NaN</td>\n","      <td>absorb</td>\n","      <td>He absorbed the costs for the accident .</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>tax</td>\n","      <td>NaN</td>\n","      <td>absorb</td>\n","      <td>The sales tax is absorbed into the state inco...</td>\n","      <td>4</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>immigrant</td>\n","      <td>NaN</td>\n","      <td>absorb</td>\n","      <td>The immigrants were quickly absorbed into soc...</td>\n","      <td>4</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>interest</td>\n","      <td>NaN</td>\n","      <td>absorb</td>\n","      <td>Her interest in butterflies absorbs her compl...</td>\n","      <td>4</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        arg1  arg2  ... verb_idx label\n","0  knowledge   NaN  ...        1     1\n","1       cost   NaN  ...        1     1\n","2        tax   NaN  ...        4     1\n","3  immigrant   NaN  ...        4     1\n","4   interest   NaN  ...        4     1\n","\n","[5 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"BQGRN8y2_Mq1"},"source":["#if label was not numeric\n","#from sklearn.preprocessing import LabelEncoder\n","\n","#encoder = LabelEncoder()\n","#df.label = encoder.fit_transform(df.label)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jdw6bWex_MuB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624829964760,"user_tz":-120,"elapsed":24,"user":{"displayName":"Giorgio Ottolina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiHqc78Tp0LGpgqAFdZiknA8pFcmI4JwnryQM6I=s64","userId":"08747627632687137907"}},"outputId":"5f9c0429-0088-4658-c938-6c0629a16de2"},"source":["# Get the lists of sentences and their labels.\n","sentences = df.sentence.values\n","labels = df.label.values\n","labels"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n","       0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,\n","       1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,\n","       0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n","       0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,\n","       1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,\n","       1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,\n","       1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,\n","       1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,\n","       1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,\n","       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,\n","       0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,\n","       1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,\n","       0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n","       0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n","       1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,\n","       1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,\n","       1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,\n","       0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,\n","       0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n","       0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,\n","       0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,\n","       1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n","       0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,\n","       0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","       0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,\n","       1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,\n","       1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,\n","       0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,\n","       0, 0, 0, 1, 0, 0, 1, 0, 0])"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"Gkx8ObbNcTUZ"},"source":["#Tokenizer"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fd_lJqo3cncS","executionInfo":{"status":"ok","timestamp":1624829967489,"user_tz":-120,"elapsed":2749,"user":{"displayName":"Giorgio Ottolina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiHqc78Tp0LGpgqAFdZiknA8pFcmI4JwnryQM6I=s64","userId":"08747627632687137907"}},"outputId":"905bf861-281b-490b-9e61-2f8f99729cff"},"source":["!pip install sentencepiece"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9uz-SE4L_MxE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624829968616,"user_tz":-120,"elapsed":1153,"user":{"displayName":"Giorgio Ottolina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiHqc78Tp0LGpgqAFdZiknA8pFcmI4JwnryQM6I=s64","userId":"08747627632687137907"}},"outputId":"df145ebb-8237-484a-e287-9c80c82601d8"},"source":["from transformers import XLMRobertaTokenizer\n","\n","# Load the BERT tokenizer.\n","print('Loading XLMRobertaTokenizer ...')\n","tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base', do_lower_case=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading XLMRobertaTokenizer ...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Zvi2HDlx_M0k","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624829968623,"user_tz":-120,"elapsed":27,"user":{"displayName":"Giorgio Ottolina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiHqc78Tp0LGpgqAFdZiknA8pFcmI4JwnryQM6I=s64","userId":"08747627632687137907"}},"outputId":"52650201-2f3b-4c67-b290-e4c2eba4d43f"},"source":["# Print the original sentence.\n","print('Original: ', sentences[0])\n","\n","# Print the sentence split into tokens.\n","print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n","\n","# Print the sentence mapped to token ids.\n","print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Original:   He absorbed the knowledge or beliefs of his tribe .\n","Tokenized:  ['▁he', '▁absorb', 'ed', '▁the', '▁knowledge', '▁or', '▁belief', 's', '▁of', '▁his', '▁tri', 'be', '▁', '.']\n","Token IDs:  [764, 57622, 297, 70, 51359, 707, 144239, 7, 111, 1919, 1927, 372, 6, 5]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tdH-JjAyev73"},"source":["#Tokenize Dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pvQC4TbTcveP","executionInfo":{"status":"ok","timestamp":1624829968625,"user_tz":-120,"elapsed":22,"user":{"displayName":"Giorgio Ottolina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiHqc78Tp0LGpgqAFdZiknA8pFcmI4JwnryQM6I=s64","userId":"08747627632687137907"}},"outputId":"6bc4674e-d58a-4fee-cbc2-c1e547bc9b2a"},"source":["# Tokenize all of the sentences and map the tokens to thier word IDs.\n","input_ids = []\n","attention_masks = []\n","\n","# For every sentence...\n","for sent in sentences:\n","    # `encode_plus` will:\n","    #   (1) Tokenize the sentence.\n","    #   (2) Prepend the `[CLS]` token to the start.\n","    #   (3) Append the `[SEP]` token to the end.\n","    #   (4) Map tokens to their IDs.\n","    #   (5) Pad or truncate the sentence to `max_length`\n","    #   (6) Create attention masks for [PAD] tokens.\n","    encoded_dict = tokenizer.encode_plus(\n","                        sent,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = 128,          # Pad & truncate all sentences.\n","                        pad_to_max_length = True,\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","    \n","    # Add the encoded sentence to the list.    \n","    input_ids.append(encoded_dict['input_ids'])\n","    \n","    # And its attention mask (simply differentiates padding from non-padding).\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","# Convert the lists into tensors.\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(labels)\n","\n","# Print sentence 0, now as a list of IDs.\n","print('Original: ', sentences[0])\n","print('Token IDs:', input_ids[0])\n","print('labels:', labels)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Original:   He absorbed the knowledge or beliefs of his tribe .\n","Token IDs: tensor([     0,    764,  57622,    297,     70,  51359,    707, 144239,      7,\n","           111,   1919,   1927,    372,      6,      5,      2,      1,      1,\n","             1,      1,      1,      1,      1,      1,      1,      1,      1,\n","             1,      1,      1,      1,      1,      1,      1,      1,      1,\n","             1,      1,      1,      1,      1,      1,      1,      1,      1,\n","             1,      1,      1,      1,      1,      1,      1,      1,      1,\n","             1,      1,      1,      1,      1,      1,      1,      1,      1,\n","             1,      1,      1,      1,      1,      1,      1,      1,      1,\n","             1,      1,      1,      1,      1,      1,      1,      1,      1,\n","             1,      1,      1,      1,      1,      1,      1,      1,      1,\n","             1,      1,      1,      1,      1,      1,      1,      1,      1,\n","             1,      1,      1,      1,      1,      1,      1,      1,      1,\n","             1,      1,      1,      1,      1,      1,      1,      1,      1,\n","             1,      1,      1,      1,      1,      1,      1,      1,      1,\n","             1,      1])\n","labels: tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,\n","        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,\n","        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,\n","        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n","        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,\n","        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,\n","        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,\n","        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,\n","        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,\n","        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,\n","        1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,\n","        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,\n","        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n","        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,\n","        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,\n","        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,\n","        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,\n","        0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,\n","        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,\n","        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,\n","        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,\n","        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,\n","        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n","        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,\n","        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,\n","        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,\n","        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dgHZenrtf4uH"},"source":["#Train and validation split"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jfrqA7YHcviX","executionInfo":{"status":"ok","timestamp":1624829968626,"user_tz":-120,"elapsed":18,"user":{"displayName":"Giorgio Ottolina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiHqc78Tp0LGpgqAFdZiknA8pFcmI4JwnryQM6I=s64","userId":"08747627632687137907"}},"outputId":"c35f98ae-8315-4732-e859-82701f5f2197"},"source":["from torch.utils.data import TensorDataset, random_split\n","\n","# Combine the training inputs into a TensorDataset.\n","dataset = TensorDataset(input_ids, attention_masks, labels)\n","\n","# Create a 90-10 train-validation split.\n","\n","# Calculate the number of samples to include in each set.\n","train_size = int(0.9 * len(dataset))\n","val_size = len(dataset) - train_size\n","\n","# Divide the dataset by randomly selecting samples.\n","train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n","\n","print('{:>5,} training samples'.format(train_size))\n","print('{:>5,} validation samples'.format(val_size))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["  582 training samples\n","   65 validation samples\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ew-crkiKcvmk"},"source":["from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","# The DataLoader needs to know our batch size for training, so we specify it \n","# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n","# size of 16 or 32.\n","batch_size = 32\n","\n","# Create the DataLoaders for our training and validation sets.\n","# We'll take training samples in random order. \n","train_dataloader = DataLoader(\n","            train_dataset,  # The training samples.\n","            sampler = RandomSampler(train_dataset), # Select batches randomly\n","            batch_size = batch_size # Trains with this batch size.\n","        )\n","\n","# For validation the order doesn't matter, so we'll just read them sequentially.\n","validation_dataloader = DataLoader(\n","            val_dataset, # The validation samples.\n","            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n","            batch_size = batch_size # Evaluate with this batch size.\n","        )\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"31XYmBgGgLMq"},"source":["#Train the model - XLMRobertaForSequenceClassification"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NCwrwWq3gKVJ","executionInfo":{"status":"ok","timestamp":1624829984049,"user_tz":-120,"elapsed":15435,"user":{"displayName":"Giorgio Ottolina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiHqc78Tp0LGpgqAFdZiknA8pFcmI4JwnryQM6I=s64","userId":"08747627632687137907"}},"outputId":"21016542-c79c-49a5-87c8-8416f36b936f"},"source":["from transformers import XLMRobertaForSequenceClassification, AdamW, BertConfig\n","\n","# Load BertForSequenceClassification - pretrained BERT model with a single linear classification layer on top. \n","model = XLMRobertaForSequenceClassification.from_pretrained(\n","    \"xlm-roberta-base\", # Use the 12-layer BERT model, with an uncased vocab.\n","    num_labels = 2, # The number of output labels--2 for binary classification.\n","                    # You can increase this for multi-class tasks.   \n","    output_attentions = False, # Whether the model returns attentions weights.\n","    output_hidden_states = False, # Whether the model returns all hidden-states.\n",")\n","\n","# Tell pytorch to run this model on the GPU.\n","model.cuda()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["XLMRobertaForSequenceClassification(\n","  (roberta): RobertaModel(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n","      (position_embeddings): Embedding(514, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (classifier): RobertaClassificationHead(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vMSwxx0gcvqh","executionInfo":{"status":"ok","timestamp":1624829984050,"user_tz":-120,"elapsed":33,"user":{"displayName":"Giorgio Ottolina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiHqc78Tp0LGpgqAFdZiknA8pFcmI4JwnryQM6I=s64","userId":"08747627632687137907"}},"outputId":"d6b2a571-162b-48d5-e3e6-b8a35ae42100"},"source":["params = list(model.named_parameters())\n","\n","print('The XLMRoberta model has {:} different named parameters.\\n'.format(len(params)))\n","\n","print('==== Embedding Layer ====\\n')\n","\n","for p in params[0:5]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","\n","print('\\n==== First Transformer ====\\n')\n","\n","for p in params[5:21]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","\n","print('\\n==== Output Layer ====\\n')\n","\n","for p in params[-4:]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The XLMRoberta model has 203 different named parameters.\n","\n","==== Embedding Layer ====\n","\n","roberta.embeddings.word_embeddings.weight               (250002, 768)\n","roberta.embeddings.position_embeddings.weight             (514, 768)\n","roberta.embeddings.token_type_embeddings.weight             (1, 768)\n","roberta.embeddings.LayerNorm.weight                           (768,)\n","roberta.embeddings.LayerNorm.bias                             (768,)\n","\n","==== First Transformer ====\n","\n","roberta.encoder.layer.0.attention.self.query.weight       (768, 768)\n","roberta.encoder.layer.0.attention.self.query.bias             (768,)\n","roberta.encoder.layer.0.attention.self.key.weight         (768, 768)\n","roberta.encoder.layer.0.attention.self.key.bias               (768,)\n","roberta.encoder.layer.0.attention.self.value.weight       (768, 768)\n","roberta.encoder.layer.0.attention.self.value.bias             (768,)\n","roberta.encoder.layer.0.attention.output.dense.weight     (768, 768)\n","roberta.encoder.layer.0.attention.output.dense.bias           (768,)\n","roberta.encoder.layer.0.attention.output.LayerNorm.weight       (768,)\n","roberta.encoder.layer.0.attention.output.LayerNorm.bias       (768,)\n","roberta.encoder.layer.0.intermediate.dense.weight        (3072, 768)\n","roberta.encoder.layer.0.intermediate.dense.bias              (3072,)\n","roberta.encoder.layer.0.output.dense.weight              (768, 3072)\n","roberta.encoder.layer.0.output.dense.bias                     (768,)\n","roberta.encoder.layer.0.output.LayerNorm.weight               (768,)\n","roberta.encoder.layer.0.output.LayerNorm.bias                 (768,)\n","\n","==== Output Layer ====\n","\n","classifier.dense.weight                                   (768, 768)\n","classifier.dense.bias                                         (768,)\n","classifier.out_proj.weight                                  (2, 768)\n","classifier.out_proj.bias                                        (2,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"51Pe3nq8g3wB"},"source":["#Optimizer and Learning Rate Scheduler"]},{"cell_type":"code","metadata":{"id":"xWkNQFlVcvup"},"source":["# Note: AdamW is a class from the huggingface library (as opposed to pytorch) - \"W\" stands for weight decay fix\n","optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5, # args.learning_rate - default is 5e-5\n","                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n","                )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mtGiVJvNhALg"},"source":["from transformers import get_linear_schedule_with_warmup\n","\n","# Number of training epochs. The BERT authors recommend between 2 and 4. \n","# We chose to run for 4, but we'll see later that this may be over-fitting the\n","# training data.\n","epochs = 10\n","\n","# Total number of training steps is [number of batches] x [number of epochs]. \n","# (Note that this is not the same as the number of training samples).\n","total_steps = len(train_dataloader) * epochs\n","\n","# Create the learning rate scheduler.\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0, # Default value in run_glue.py\n","                                            num_training_steps = total_steps)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K3a_KwCxhIw4"},"source":["#Train our model"]},{"cell_type":"code","metadata":{"id":"qZsMe3FshAPv"},"source":["import numpy as np\n","\n","# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eIoz0srmhAZR"},"source":["import time\n","import datetime\n","\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Uf5f0hyehAhP","executionInfo":{"status":"ok","timestamp":1624830029269,"user_tz":-120,"elapsed":45242,"user":{"displayName":"Giorgio Ottolina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiHqc78Tp0LGpgqAFdZiknA8pFcmI4JwnryQM6I=s64","userId":"08747627632687137907"}},"outputId":"d13a3d74-90c2-4df5-af10-f5e791b40dc4"},"source":["import random\n","import numpy as np\n","\n","# This training code is based on the `run_glue.py` script here:\n","# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n","\n","# Set the seed value all over the place to make this reproducible.\n","seed_val = 42\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# We'll store a number of quantities such as training and validation loss, \n","# validation accuracy, and timings.\n","training_stats = []\n","\n","# Measure the total training time for the whole run.\n","total_t0 = time.time()\n","\n","# For each epoch...\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    # Perform one full pass over the training set.\n","\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # Measure how long the training epoch takes.\n","    t0 = time.time()\n","\n","    # Reset the total loss for this epoch.\n","    total_train_loss = 0\n","\n","    # Put the model into training mode. Don't be mislead--the call to \n","    # `train` just changes the *mode*, it doesn't *perform* the training.\n","    # `dropout` and `batchnorm` layers behave differently during training\n","    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n","    model.train()\n","\n","    # For each batch of training data...\n","    for step, batch in enumerate(train_dataloader):\n","\n","        # Progress update every 40 batches.\n","        if step % 40 == 0 and not step == 0:\n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)\n","            \n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n","        # `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels \n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","        print(b_input_ids.shape)\n","        print(b_input_mask.shape)\n","        print(b_labels.shape)\n","\n","        # Always clear any previously calculated gradients before performing a\n","        # backward pass. PyTorch doesn't do this automatically because \n","        # accumulating the gradients is \"convenient while training RNNs\". \n","        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n","        model.zero_grad()        \n","\n","        # Perform a forward pass (evaluate the model on this training batch).\n","        # The documentation for this `model` function is here: \n","        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","        # It returns different numbers of parameters depending on what arguments\n","        # arge given and what flags are set. For our useage here, it returns\n","        # the loss (because we provided labels) and the \"logits\"--the model\n","        # outputs prior to activation.\n","        loss, logits = model(b_input_ids, \n","                             token_type_ids=None, \n","                             attention_mask=b_input_mask, \n","                             labels=b_labels)\n","\n","        # Accumulate the training loss over all of the batches so that we can\n","        # calculate the average loss at the end. `loss` is a Tensor containing a\n","        # single value; the `.item()` function just returns the Python value \n","        # from the tensor.\n","        total_train_loss += loss.item()\n","\n","        # Perform a backward pass to calculate the gradients.\n","        loss.backward()\n","\n","        # Clip the norm of the gradients to 1.0.\n","        # This is to help prevent the \"exploding gradients\" problem.\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # Update parameters and take a step using the computed gradient.\n","        # The optimizer dictates the \"update rule\"--how the parameters are\n","        # modified based on their gradients, the learning rate, etc.\n","        optimizer.step()\n","\n","        # Update the learning rate.\n","        scheduler.step()\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_train_loss = total_train_loss / len(train_dataloader)            \n","    \n","    # Measure how long this epoch took.\n","    training_time = format_time(time.time() - t0)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epcoh took: {:}\".format(training_time))\n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    # After the completion of each training epoch, measure our performance on\n","    # our validation set.\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    t0 = time.time()\n","\n","    # Put the model in evaluation mode--the dropout layers behave differently\n","    # during evaluation.\n","    model.eval()\n","\n","    # Tracking variables \n","    total_eval_accuracy = 0\n","    total_eval_loss = 0\n","    nb_eval_steps = 0\n","\n","    # Evaluate data for one epoch\n","    for batch in validation_dataloader:\n","        \n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using \n","        # the `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels \n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","        \n","        # Tell pytorch not to bother with constructing the compute graph during\n","        # the forward pass, since this is only needed for backprop (training).\n","        with torch.no_grad():        \n","\n","            # Forward pass, calculate logit predictions.\n","            # token_type_ids is the same as the \"segment ids\", which \n","            # differentiates sentence 1 and 2 in 2-sentence tasks.\n","            # The documentation for this `model` function is here: \n","            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","            # Get the \"logits\" output by the model. The \"logits\" are the output\n","            # values prior to applying an activation function like the softmax.\n","            (loss, logits) = model(b_input_ids, \n","                                   token_type_ids=None, \n","                                   attention_mask=b_input_mask,\n","                                   labels=b_labels)\n","            \n","        # Accumulate the validation loss.\n","        total_eval_loss += loss.item()\n","\n","        # Move logits and labels to CPU\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","\n","        # Calculate the accuracy for this batch of test sentences, and\n","        # accumulate it over all batches.\n","        total_eval_accuracy += flat_accuracy(logits, label_ids)\n","        \n","\n","    # Report the final accuracy for this validation run.\n","    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n","    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_val_loss = total_eval_loss / len(validation_dataloader)\n","    \n","    # Measure how long the validation run took.\n","    validation_time = format_time(time.time() - t0)\n","    \n","    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n","    print(\"  Validation took: {:}\".format(validation_time))\n","\n","    # Record all statistics from this epoch.\n","    training_stats.append(\n","        {\n","            'epoch': epoch_i + 1,\n","            'Training Loss': avg_train_loss,\n","            'Valid. Loss': avg_val_loss,\n","            'Valid. Accur.': avg_val_accuracy,\n","            'Training Time': training_time,\n","            'Validation Time': validation_time\n","        }\n","    )\n","\n","print(\"\")\n","print(\"Training complete!\")\n","\n","print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","======== Epoch 1 / 10 ========\n","Training...\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([6, 128])\n","torch.Size([6, 128])\n","torch.Size([6])\n","\n","  Average training loss: 0.70\n","  Training epcoh took: 0:00:04\n","\n","Running Validation...\n","  Accuracy: 0.68\n","  Validation Loss: 0.68\n","  Validation took: 0:00:00\n","\n","======== Epoch 2 / 10 ========\n","Training...\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([6, 128])\n","torch.Size([6, 128])\n","torch.Size([6])\n","\n","  Average training loss: 0.66\n","  Training epcoh took: 0:00:04\n","\n","Running Validation...\n","  Accuracy: 0.52\n","  Validation Loss: 0.64\n","  Validation took: 0:00:00\n","\n","======== Epoch 3 / 10 ========\n","Training...\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([6, 128])\n","torch.Size([6, 128])\n","torch.Size([6])\n","\n","  Average training loss: 0.61\n","  Training epcoh took: 0:00:04\n","\n","Running Validation...\n","  Accuracy: 0.54\n","  Validation Loss: 0.70\n","  Validation took: 0:00:00\n","\n","======== Epoch 4 / 10 ========\n","Training...\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([6, 128])\n","torch.Size([6, 128])\n","torch.Size([6])\n","\n","  Average training loss: 0.51\n","  Training epcoh took: 0:00:04\n","\n","Running Validation...\n","  Accuracy: 0.53\n","  Validation Loss: 0.72\n","  Validation took: 0:00:00\n","\n","======== Epoch 5 / 10 ========\n","Training...\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([6, 128])\n","torch.Size([6, 128])\n","torch.Size([6])\n","\n","  Average training loss: 0.46\n","  Training epcoh took: 0:00:04\n","\n","Running Validation...\n","  Accuracy: 0.53\n","  Validation Loss: 1.04\n","  Validation took: 0:00:00\n","\n","======== Epoch 6 / 10 ========\n","Training...\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([6, 128])\n","torch.Size([6, 128])\n","torch.Size([6])\n","\n","  Average training loss: 0.42\n","  Training epcoh took: 0:00:04\n","\n","Running Validation...\n","  Accuracy: 0.54\n","  Validation Loss: 1.07\n","  Validation took: 0:00:00\n","\n","======== Epoch 7 / 10 ========\n","Training...\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([6, 128])\n","torch.Size([6, 128])\n","torch.Size([6])\n","\n","  Average training loss: 0.38\n","  Training epcoh took: 0:00:04\n","\n","Running Validation...\n","  Accuracy: 0.54\n","  Validation Loss: 1.25\n","  Validation took: 0:00:00\n","\n","======== Epoch 8 / 10 ========\n","Training...\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([6, 128])\n","torch.Size([6, 128])\n","torch.Size([6])\n","\n","  Average training loss: 0.38\n","  Training epcoh took: 0:00:04\n","\n","Running Validation...\n","  Accuracy: 0.52\n","  Validation Loss: 1.39\n","  Validation took: 0:00:00\n","\n","======== Epoch 9 / 10 ========\n","Training...\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([6, 128])\n","torch.Size([6, 128])\n","torch.Size([6])\n","\n","  Average training loss: 0.36\n","  Training epcoh took: 0:00:04\n","\n","Running Validation...\n","  Accuracy: 0.52\n","  Validation Loss: 1.42\n","  Validation took: 0:00:00\n","\n","======== Epoch 10 / 10 ========\n","Training...\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([32, 128])\n","torch.Size([32, 128])\n","torch.Size([32])\n","torch.Size([6, 128])\n","torch.Size([6, 128])\n","torch.Size([6])\n","\n","  Average training loss: 0.32\n","  Training epcoh took: 0:00:04\n","\n","Running Validation...\n","  Accuracy: 0.53\n","  Validation Loss: 1.36\n","  Validation took: 0:00:00\n","\n","Training complete!\n","Total training took 0:00:45 (h:mm:ss)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":393},"id":"LHx9Nzi9hAn_","executionInfo":{"status":"ok","timestamp":1624830029271,"user_tz":-120,"elapsed":30,"user":{"displayName":"Giorgio Ottolina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiHqc78Tp0LGpgqAFdZiknA8pFcmI4JwnryQM6I=s64","userId":"08747627632687137907"}},"outputId":"604a6c53-a043-4c04-a300-85fce9800f2f"},"source":["import pandas as pd\n","\n","# Display floats with two decimal places.\n","pd.set_option('precision', 2)\n","\n","# Create a DataFrame from our training statistics.\n","df_stats = pd.DataFrame(data=training_stats)\n","\n","# Use the 'epoch' as the row index.\n","df_stats = df_stats.set_index('epoch')\n","\n","# A hack to force the column headers to wrap.\n","#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n","\n","# Display the table.\n","df_stats"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Training Loss</th>\n","      <th>Valid. Loss</th>\n","      <th>Valid. Accur.</th>\n","      <th>Training Time</th>\n","      <th>Validation Time</th>\n","    </tr>\n","    <tr>\n","      <th>epoch</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>0.70</td>\n","      <td>0.68</td>\n","      <td>0.68</td>\n","      <td>0:00:04</td>\n","      <td>0:00:00</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.66</td>\n","      <td>0.64</td>\n","      <td>0.52</td>\n","      <td>0:00:04</td>\n","      <td>0:00:00</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.61</td>\n","      <td>0.70</td>\n","      <td>0.54</td>\n","      <td>0:00:04</td>\n","      <td>0:00:00</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.51</td>\n","      <td>0.72</td>\n","      <td>0.53</td>\n","      <td>0:00:04</td>\n","      <td>0:00:00</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0.46</td>\n","      <td>1.04</td>\n","      <td>0.53</td>\n","      <td>0:00:04</td>\n","      <td>0:00:00</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>0.42</td>\n","      <td>1.07</td>\n","      <td>0.54</td>\n","      <td>0:00:04</td>\n","      <td>0:00:00</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>0.38</td>\n","      <td>1.25</td>\n","      <td>0.54</td>\n","      <td>0:00:04</td>\n","      <td>0:00:00</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>0.38</td>\n","      <td>1.39</td>\n","      <td>0.52</td>\n","      <td>0:00:04</td>\n","      <td>0:00:00</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>0.36</td>\n","      <td>1.42</td>\n","      <td>0.52</td>\n","      <td>0:00:04</td>\n","      <td>0:00:00</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>0.32</td>\n","      <td>1.36</td>\n","      <td>0.53</td>\n","      <td>0:00:04</td>\n","      <td>0:00:00</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n","epoch                                                                         \n","1               0.70         0.68           0.68       0:00:04         0:00:00\n","2               0.66         0.64           0.52       0:00:04         0:00:00\n","3               0.61         0.70           0.54       0:00:04         0:00:00\n","4               0.51         0.72           0.53       0:00:04         0:00:00\n","5               0.46         1.04           0.53       0:00:04         0:00:00\n","6               0.42         1.07           0.54       0:00:04         0:00:00\n","7               0.38         1.25           0.54       0:00:04         0:00:00\n","8               0.38         1.39           0.52       0:00:04         0:00:00\n","9               0.36         1.42           0.52       0:00:04         0:00:00\n","10              0.32         1.36           0.53       0:00:04         0:00:00"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":427},"id":"d9EJhSWFhAxL","executionInfo":{"status":"ok","timestamp":1624830030154,"user_tz":-120,"elapsed":910,"user":{"displayName":"Giorgio Ottolina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiHqc78Tp0LGpgqAFdZiknA8pFcmI4JwnryQM6I=s64","userId":"08747627632687137907"}},"outputId":"579f6f11-0ca6-49b6-9e6a-638a0a73d85b"},"source":["import matplotlib.pyplot as plt\n","% matplotlib inline\n","\n","import seaborn as sns\n","\n","# Use plot styling from seaborn.\n","sns.set(style='darkgrid')\n","\n","# Increase the plot size and font size.\n","sns.set(font_scale=1.5)\n","plt.rcParams[\"figure.figsize\"] = (12,6)\n","\n","# Plot the learning curve.\n","plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n","plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n","\n","# Label the plot.\n","plt.title(\"Training & Validation Loss\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.xticks([1, 2, 3, 4])\n","\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 864x432 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"IT4-LcM-iPn8"},"source":["#Performance on test set"]},{"cell_type":"code","metadata":{"id":"8VipplfqhBhS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624830030157,"user_tz":-120,"elapsed":79,"user":{"displayName":"Giorgio Ottolina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiHqc78Tp0LGpgqAFdZiknA8pFcmI4JwnryQM6I=s64","userId":"08747627632687137907"}},"outputId":"66943db2-9215-4ba4-ee26-9b48a51dec13"},"source":["import pandas as pd\n","\n","# # Load the dataset into a pandas dataframe.\n","#test_df = pd.read_csv(\"Datasets/es_lcc_new.csv\")\n","\n","# # Report the number of sentences.\n","print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n","\n","# # Create sentence and label lists\n","sentences = test_df.sentence.values.astype(str)\n","labels = test_df.label.values\n","\n","# # Tokenize all of the sentences and map the tokens to thier word IDs.\n","input_ids = []\n","attention_masks = []\n","\n","# # For every sentence...\n","for sent in sentences:\n","#     # `encode_plus` will:\n","#     #   (1) Tokenize the sentence.\n","#     #   (2) Prepend the `[CLS]` token to the start.\n","#     #   (3) Append the `[SEP]` token to the end.\n","#     #   (4) Map tokens to their IDs.\n","#     #   (5) Pad or truncate the sentence to `max_length`\n","#     #   (6) Create attention masks for [PAD] tokens.\n","     encoded_dict = tokenizer.encode_plus(\n","                         sent,                      # Sentence to encode.\n","                         add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                         max_length = 64,           # Pad & truncate all sentences.\n","                         pad_to_max_length = True,\n","                         return_attention_mask = True,   # Construct attn. masks.\n","                         return_tensors = 'pt',     # Return pytorch tensors.\n","                    )\n","    \n","#     # Add the encoded sentence to the list.    \n","     input_ids.append(encoded_dict['input_ids'])\n","    \n","#     # And its attention mask (simply differentiates padding from non-padding).\n","     attention_masks.append(encoded_dict['attention_mask'])\n","\n","# # Convert the lists into tensors.\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(labels)\n","\n","# # Set the batch size.  \n","batch_size = 32  \n","\n","# # Create the DataLoader.\n","prediction_data = TensorDataset(input_ids, attention_masks, labels)\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of test sentences: 647\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HLjiQA_TiUbi"},"source":["#Evaluation on test set"]},{"cell_type":"code","metadata":{"id":"Gnv1WjdwhBrg"},"source":["# # Prediction on test set\n","\n","# print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n","\n","# # Put model in evaluation mode\n","# model.eval()\n","\n","# # Tracking variables \n","# predictions , true_labels = [], []\n","\n","# # Predict \n","# for batch in prediction_dataloader:\n","#   # Add batch to GPU\n","#   batch = tuple(t.to(device) for t in batch)\n","  \n","#   # Unpack the inputs from our dataloader\n","#   b_input_ids, b_input_mask, b_labels = batch\n","  \n","#   # Telling the model not to compute or store gradients, saving memory and \n","#   # speeding up prediction\n","#   with torch.no_grad():\n","#       # Forward pass, calculate logit predictions\n","#       outputs = model(b_input_ids, token_type_ids=None, \n","#                       attention_mask=b_input_mask)\n","\n","#   logits = outputs[0]\n","\n","#   # Move logits and labels to CPU\n","#   logits = logits.detach().cpu().numpy()\n","#   label_ids = b_labels.to('cpu').numpy()\n","  \n","#   # Store predictions and true labels\n","#   predictions.append(logits)\n","#   true_labels.append(label_ids)\n","\n","\n","# print('    DONE.')\n","# print('    predictions:::',predictions)\n","# print('    true_labels:::',true_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qmj7fm818zxM"},"source":["model.eval()\n","\n","# Tracking variables \n","predictions , true_labels = [], []\n","\n","# Predict \n","for batch in prediction_dataloader:\n","  # Add batch to GPU\n","  batch = tuple(t.to(device) for t in batch)\n","  # Unpack the inputs from our dataloader\n","  b_input_ids, b_input_mask, b_labels = batch\n","  # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n","  with torch.no_grad():\n","    # Forward pass, calculate logit predictions\n","    logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)[0]\n","\n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","  \n","  # Store predictions and true labels\n","  predictions.append(logits)\n","  true_labels.append(label_ids)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WfsjU8Upt38K"},"source":["my_submission = pd.DataFrame()\n","my_submission['sentence'] = test_df['sentence']\n","my_submission['arg1'] = test_df['arg1']\n","my_submission['verb'] = test_df['verb']\n","my_submission['correct_label'] = test_df['label']\n","#my_submission['polarity'] = test_df['polarity']\n","#my_submission['intensity'] = test_df['intensity']\n","#my_submission['source_concept'] = test_df['source_concept']\n","#my_submission['target_concept'] = test_df['target_concept']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VNV-BxYnuNZh"},"source":["final_preds = []\n","for p in predictions:\n","    for i in p:\n","        final_preds.append(np.argmax(i))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JN1eyJlFuPCc"},"source":["my_submission['label'] = final_preds"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FDLPomjZuR7W"},"source":["my_submission['label'] = my_submission['label'].map({0:0, 1:1})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fqo58IR-ufCG","colab":{"base_uri":"https://localhost:8080/","height":205},"executionInfo":{"status":"ok","timestamp":1624830030166,"user_tz":-120,"elapsed":79,"user":{"displayName":"Giorgio Ottolina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiHqc78Tp0LGpgqAFdZiknA8pFcmI4JwnryQM6I=s64","userId":"08747627632687137907"}},"outputId":"79eb9299-4b3a-4bfa-ba87-27b12e1d33da"},"source":["my_submission.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence</th>\n","      <th>arg1</th>\n","      <th>verb</th>\n","      <th>correct_label</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>634</th>\n","      <td>He marched into the classroom and announced t...</td>\n","      <td>man</td>\n","      <td>march</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>220</th>\n","      <td>The stars gravitate towards each other .</td>\n","      <td>star</td>\n","      <td>gravitate</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>426</th>\n","      <td>A hot soup will revive me .</td>\n","      <td>soup</td>\n","      <td>revive</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>428</th>\n","      <td>He revived this style of opera .</td>\n","      <td>style</td>\n","      <td>revive</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>72</th>\n","      <td>They clawed their way to the top of the mount...</td>\n","      <td>way</td>\n","      <td>claw</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                              sentence  ... label\n","634   He marched into the classroom and announced t...  ...     0\n","220           The stars gravitate towards each other .  ...     0\n","426                        A hot soup will revive me .  ...     1\n","428                   He revived this style of opera .  ...     1\n","72    They clawed their way to the top of the mount...  ...     0\n","\n","[5 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"id":"YQs-dWrUw7XN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624830030167,"user_tz":-120,"elapsed":75,"user":{"displayName":"Giorgio Ottolina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiHqc78Tp0LGpgqAFdZiknA8pFcmI4JwnryQM6I=s64","userId":"08747627632687137907"}},"outputId":"ede9fbe4-0e02-409c-ae6e-169a13f6239c"},"source":["my_submission.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(65, 5)"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"code","metadata":{"id":"KsIV4fzxxttP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624830030168,"user_tz":-120,"elapsed":69,"user":{"displayName":"Giorgio Ottolina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiHqc78Tp0LGpgqAFdZiknA8pFcmI4JwnryQM6I=s64","userId":"08747627632687137907"}},"outputId":"3d46b7a9-887a-471c-c25f-56085adc953e"},"source":["test_df.label.value_counts()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1    36\n","0    29\n","Name: label, dtype: int64"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"code","metadata":{"id":"nTtwGl9vxOoG"},"source":["final = my_submission[(my_submission['correct_label'] == my_submission['label'])]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6iVw1NZ2xO0C","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624830030173,"user_tz":-120,"elapsed":69,"user":{"displayName":"Giorgio Ottolina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiHqc78Tp0LGpgqAFdZiknA8pFcmI4JwnryQM6I=s64","userId":"08747627632687137907"}},"outputId":"0eb7dab3-7c7a-4ff7-f502-448dd9d198f4"},"source":["final.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(57, 5)"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"code","metadata":{"id":"1BoXX0koxO6n"},"source":["final_met = my_submission[(my_submission['correct_label'] == 1) & (my_submission['label'] ==1)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zQGkR9dDxeSg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624830030178,"user_tz":-120,"elapsed":66,"user":{"displayName":"Giorgio Ottolina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiHqc78Tp0LGpgqAFdZiknA8pFcmI4JwnryQM6I=s64","userId":"08747627632687137907"}},"outputId":"b7e5c7c9-3240-4785-903b-21e1f0190c2f"},"source":["final_met.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(31, 5)"]},"metadata":{"tags":[]},"execution_count":39}]},{"cell_type":"code","metadata":{"id":"RFTH_BCexeY1"},"source":["final_lit = my_submission[(my_submission['correct_label'] == 0) & (my_submission['label'] ==0)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UEwct9X5xehQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624830030182,"user_tz":-120,"elapsed":63,"user":{"displayName":"Giorgio Ottolina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiHqc78Tp0LGpgqAFdZiknA8pFcmI4JwnryQM6I=s64","userId":"08747627632687137907"}},"outputId":"1c08452e-1eb9-49cf-bd98-c7cc3fe22e5b"},"source":["final_lit.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(26, 5)"]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"code","metadata":{"id":"Rs-f8IKraTz5"},"source":["#print(logits)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g7YGsSh_uhz7"},"source":["my_submission.to_csv('mohx_sub.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9GYFtiTEk7MP"},"source":["#final_met.to_csv(\"final_met.csv\", index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Whz6mWvpidb-"},"source":["#Save and load fine-tuned model"]},{"cell_type":"code","metadata":{"id":"73UumM0PhBym","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624830049865,"user_tz":-120,"elapsed":19737,"user":{"displayName":"Giorgio Ottolina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiHqc78Tp0LGpgqAFdZiknA8pFcmI4JwnryQM6I=s64","userId":"08747627632687137907"}},"outputId":"f293b4a6-970f-4b13-e25f-097f8db2f194"},"source":["import os\n","\n","# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n","\n","output_dir = 'mohx_xlmroberta/xlm-roberta_model_save'\n","# output_dir = './content/xlm-roberta_model_save/'\n","\n","# Create output directory if needed\n","if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)\n","\n","print(\"Saving model to %s\" % output_dir)\n","\n","# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n","# They can then be reloaded using `from_pretrained()`\n","model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n","model_to_save.save_pretrained(output_dir)\n","tokenizer.save_pretrained(output_dir)\n","\n","# Good practice: save your training arguments together with the trained model\n","# torch.save(args, os.path.join(output_dir, 'training_args.bin'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Saving model to mohx_xlmroberta/xlm-roberta_model_save\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["('mohx_xlmroberta/xlm-roberta_model_save/sentencepiece.bpe.model',\n"," 'mohx_xlmroberta/xlm-roberta_model_save/special_tokens_map.json',\n"," 'mohx_xlmroberta/xlm-roberta_model_save/added_tokens.json')"]},"metadata":{"tags":[]},"execution_count":45}]},{"cell_type":"markdown","metadata":{"id":"LmN96FOqjLKd"},"source":["#Import saved model and test"]},{"cell_type":"code","metadata":{"id":"ScJHWcE5hB4z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624830052392,"user_tz":-120,"elapsed":2546,"user":{"displayName":"Giorgio Ottolina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiHqc78Tp0LGpgqAFdZiknA8pFcmI4JwnryQM6I=s64","userId":"08747627632687137907"}},"outputId":"257918ed-d5a8-41a6-9ef3-5e2750cb518d"},"source":["!pip install transformers\n","\n","from transformers import XLMRobertaForSequenceClassification\n","\n","output_dir = 'mohx_xlmroberta/xlm-roberta_model_save'\n","\n","print(output_dir)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (3.0.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: tokenizers==0.8.0-rc4 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.8.0rc4)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from transformers) (0.1.96)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","mohx_xlmroberta/xlm-roberta_model_save\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SZbux55ucvy5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624830065865,"user_tz":-120,"elapsed":13481,"user":{"displayName":"Giorgio Ottolina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiHqc78Tp0LGpgqAFdZiknA8pFcmI4JwnryQM6I=s64","userId":"08747627632687137907"}},"outputId":"bbd25a65-095e-481d-cf67-01a5cdb9c1fa"},"source":["from transformers import XLMRobertaTokenizer\n","import torch\n","# Load the BERT tokenizer.\n","print('Loading XLMRobertaTokenizer...')\n","tokenizer = XLMRobertaTokenizer.from_pretrained(output_dir)\n","model_loaded = XLMRobertaForSequenceClassification.from_pretrained(output_dir)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading XLMRobertaTokenizer...\n"],"name":"stdout"}]}]}