{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"},"colab":{"name":"mohx_mlm_xlm_finetuned.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"QbOvSBgh_K2T","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626290715481,"user_tz":-120,"elapsed":386,"user":{"displayName":"Giorgio Ottolina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiHqc78Tp0LGpgqAFdZiknA8pFcmI4JwnryQM6I=s64","userId":"08747627632687137907"}},"outputId":"f96e027d-c6d6-480b-eba3-5c9fc9ec5aa7"},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vzqIeh0k_MbM"},"source":["# %%\n","import torch\n","import string"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1MxtokpADlrg","executionInfo":{"status":"ok","timestamp":1626290719383,"user_tz":-120,"elapsed":2766,"user":{"displayName":"Giorgio Ottolina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiHqc78Tp0LGpgqAFdZiknA8pFcmI4JwnryQM6I=s64","userId":"08747627632687137907"}},"outputId":"28fd2fc8-d402-4dee-eb66-343516ea7b39"},"source":["!pip install sentencepiece"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_aYg8y0jDp3Q","executionInfo":{"status":"ok","timestamp":1626290722063,"user_tz":-120,"elapsed":2692,"user":{"displayName":"Giorgio Ottolina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiHqc78Tp0LGpgqAFdZiknA8pFcmI4JwnryQM6I=s64","userId":"08747627632687137907"}},"outputId":"a22e63ae-abf1-47f0-d40e-717e89b22675"},"source":["!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.8.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"u3o7xk5lJrCO"},"source":["##BERT"]},{"cell_type":"code","metadata":{"id":"L6tTOkJV90Ny"},"source":["#from transformers import BertTokenizer, BertForMaskedLM\n","#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","#model = BertForMaskedLM.from_pretrained('bert-base-uncased')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1cJ9cXEAJtTy"},"source":["##XLM-R"]},{"cell_type":"code","metadata":{"id":"6Eq8HGAPI7z1"},"source":["from transformers import XLMRobertaTokenizer, XLMRobertaForMaskedLM\n","tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n","model = XLMRobertaForMaskedLM.from_pretrained('xlm-roberta-base')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V5QYLg3G6fY6","executionInfo":{"status":"ok","timestamp":1626290728006,"user_tz":-120,"elapsed":30,"user":{"displayName":"Giorgio Ottolina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiHqc78Tp0LGpgqAFdZiknA8pFcmI4JwnryQM6I=s64","userId":"08747627632687137907"}},"outputId":"e6f96128-46ac-4d09-b492-31145db4994b"},"source":["cd drive/My Drive/Colab Notebooks/experiments"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks/experiments\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hkFPS4ri6fcl"},"source":["# Use a subset for quick experiments\n","#data = data[:10000]\n","\n","from sklearn.model_selection import train_test_split as tts\n","import pandas as pd\n","data = pd.read_csv(\"data/moh-x.csv\")\n","\n","# Split to train, val and test\n","text, test_df = tts(data[[\"sentence\", \"arg1\", \"verb\", \"label\"]], random_state=42, test_size=0.1)\n","text, val = tts(text, random_state=42, test_size=test_df.shape[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fbLyN4G06ff-"},"source":["text = text[\"sentence\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yNLrW8b26fje"},"source":["#text.to_csv(r'mohx.txt', header=None, index=None, sep=' ', mode='a')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OaazadAn6fna"},"source":["with open('data/mohx.txt', 'r') as fp:\n","    text = fp.read().split('\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x5T7ZcUe6fqv","executionInfo":{"status":"ok","timestamp":1626290728515,"user_tz":-120,"elapsed":66,"user":{"displayName":"Giorgio Ottolina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiHqc78Tp0LGpgqAFdZiknA8pFcmI4JwnryQM6I=s64","userId":"08747627632687137907"}},"outputId":"0969aee2-9f36-4724-c590-69df7152295f"},"source":["text[:5]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['\" I ca n\\'t buy this story .\"',\n"," '\" European children learn the breast stroke ; they often do n\\'t know how to crawl .\"',\n"," '\" He shed his image as a pushy boss .\"',\n"," '\" She devoured his novels .\"',\n"," '\" Age and experience mellowed him over the years .\"']"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"_rCWmO8Q6fuE"},"source":["inputs = tokenizer(text, return_tensors='pt', max_length=21, truncation=True, padding='max_length')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ByliEW7A6fxk","executionInfo":{"status":"ok","timestamp":1626290728517,"user_tz":-120,"elapsed":53,"user":{"displayName":"Giorgio Ottolina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiHqc78Tp0LGpgqAFdZiknA8pFcmI4JwnryQM6I=s64","userId":"08747627632687137907"}},"outputId":"ac6d52fa-3d48-4c90-b5f5-b1b096d13d08"},"source":["inputs"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': tensor([[     0,     44,     87,  ...,      1,      1,      1],\n","        [     0,     44,  28811,  ..., 235879,      6,      2],\n","        [     0,     44,   1529,  ...,      1,      1,      1],\n","        ...,\n","        [     0,     44,  14838,  ...,      1,      1,      1],\n","        [     0,     44,   4687,  ...,      1,      1,      1],\n","        [     0,      2,      1,  ...,      1,      1,      1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 0,  ..., 0, 0, 0]])}"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"L4TTvenw6f0z"},"source":["inputs['labels'] = inputs.input_ids.detach().clone()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H8MoIgrV6f3_","executionInfo":{"status":"ok","timestamp":1626290728519,"user_tz":-120,"elapsed":46,"user":{"displayName":"Giorgio Ottolina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiHqc78Tp0LGpgqAFdZiknA8pFcmI4JwnryQM6I=s64","userId":"08747627632687137907"}},"outputId":"7e5e5730-8ab1-4270-8c0a-83c039a6d00a"},"source":["inputs.keys()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys(['input_ids', 'attention_mask', 'labels'])"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"pb1BUVy76f7I"},"source":["# create random array of floats with equal dimensions to input_ids tensor\n","rand = torch.rand(inputs.input_ids.shape)\n","# create mask array\n","mask_arr = (rand < 0.15) * (inputs.input_ids != 101) * \\\n","           (inputs.input_ids != 102) * (inputs.input_ids != 0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VV8yTOeJ6f_E","executionInfo":{"status":"ok","timestamp":1626290728520,"user_tz":-120,"elapsed":40,"user":{"displayName":"Giorgio Ottolina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiHqc78Tp0LGpgqAFdZiknA8pFcmI4JwnryQM6I=s64","userId":"08747627632687137907"}},"outputId":"2ebe016d-9ae0-44c3-ddb4-7144259aa451"},"source":["mask_arr"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[False, False, False,  ..., False, False, False],\n","        [False,  True, False,  ..., False, False, False],\n","        [False,  True, False,  ..., False, False, False],\n","        ...,\n","        [False, False, False,  ..., False, False, False],\n","        [False,  True, False,  ...,  True, False, False],\n","        [False, False, False,  ..., False, False,  True]])"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"x28Dy5aP6gCU"},"source":["selection = []\n","\n","for i in range(inputs.input_ids.shape[0]):\n","    selection.append(\n","        torch.flatten(mask_arr[i].nonzero()).tolist()\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VehaV81O6gFp","executionInfo":{"status":"ok","timestamp":1626290728521,"user_tz":-120,"elapsed":33,"user":{"displayName":"Giorgio Ottolina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiHqc78Tp0LGpgqAFdZiknA8pFcmI4JwnryQM6I=s64","userId":"08747627632687137907"}},"outputId":"183d6fc3-6357-430d-e1af-2c765d7044f6"},"source":["selection[:5]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[11], [1, 12], [1, 13, 14, 15], [17], [6, 7, 8]]"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"KFjuxk8O6gIt"},"source":["for i in range(inputs.input_ids.shape[0]):\n","    inputs.input_ids[i, selection[i]] = 103"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k1iv7o036gL3","executionInfo":{"status":"ok","timestamp":1626290728522,"user_tz":-120,"elapsed":28,"user":{"displayName":"Giorgio Ottolina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiHqc78Tp0LGpgqAFdZiknA8pFcmI4JwnryQM6I=s64","userId":"08747627632687137907"}},"outputId":"7576acfb-5bbd-440d-bbc5-7362763b759b"},"source":["inputs.input_ids"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[     0,     44,     87,  ...,      1,      1,      1],\n","        [     0,    103,  28811,  ..., 235879,      6,      2],\n","        [     0,    103,   1529,  ...,      1,      1,      1],\n","        ...,\n","        [     0,     44,  14838,  ...,      1,      1,      1],\n","        [     0,    103,   4687,  ...,    103,      1,      1],\n","        [     0,      2,      1,  ...,      1,      1,    103]])"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"YiOhXkXd6gO_"},"source":["class MeditationsDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings):\n","        self.encodings = encodings\n","    def __getitem__(self, idx):\n","        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","    def __len__(self):\n","        return len(self.encodings.input_ids)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"15keHTnX6gST"},"source":["dataset = MeditationsDataset(inputs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nuVurFUc6gVh"},"source":["loader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SDel2JGB6gYq","executionInfo":{"status":"ok","timestamp":1626290732273,"user_tz":-120,"elapsed":3414,"user":{"displayName":"Giorgio Ottolina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiHqc78Tp0LGpgqAFdZiknA8pFcmI4JwnryQM6I=s64","userId":"08747627632687137907"}},"outputId":"8f1c6ae3-fc4a-4af9-84ec-72a7f307a810"},"source":["device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","# and move our model over to the selected device\n","model.to(device)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["XLMRobertaForMaskedLM(\n","  (roberta): RobertaModel(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n","      (position_embeddings): Embedding(514, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (lm_head): RobertaLMHead(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    (decoder): Linear(in_features=768, out_features=250002, bias=True)\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"id":"nfztqLD_6gb4"},"source":["from transformers import AdamW\n","\n","# activate training mode\n","model.train()\n","# initialize optimizer\n","optim = AdamW(model.parameters(), lr=5e-5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PG5Jlros6ggU","executionInfo":{"status":"ok","timestamp":1626290750087,"user_tz":-120,"elapsed":16532,"user":{"displayName":"Giorgio Ottolina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiHqc78Tp0LGpgqAFdZiknA8pFcmI4JwnryQM6I=s64","userId":"08747627632687137907"}},"outputId":"540dfe7d-bfe0-4965-e909-cbcdb091f724"},"source":["from tqdm import tqdm  # for our progress bar\n","\n","epochs = 3\n","\n","for epoch in range(epochs):\n","    # setup loop with TQDM and dataloader\n","    loop = tqdm(loader, leave=True)\n","    for batch in loop:\n","        # initialize calculated gradients (from prev step)\n","        optim.zero_grad()\n","        # pull all tensor batches required for training\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","        # process\n","        outputs = model(input_ids, attention_mask=attention_mask,\n","                        labels=labels)\n","        # extract loss\n","        loss = outputs.loss\n","        # calculate loss for every parameter that needs grad update\n","        loss.backward()\n","        # update parameters\n","        optim.step()\n","        # print relevant info to progress bar\n","        loop.set_description(f'Epoch {epoch}')\n","        loop.set_postfix(loss=loss.item())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["  0%|          | 0/33 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  \"\"\"\n","Epoch 0: 100%|██████████| 33/33 [00:05<00:00,  6.00it/s, loss=0.72]\n","Epoch 1: 100%|██████████| 33/33 [00:05<00:00,  6.01it/s, loss=0.654]\n","Epoch 2: 100%|██████████| 33/33 [00:05<00:00,  6.02it/s, loss=0.0586]\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"FaFKkaKcJ245"},"source":["##Save Model"]},{"cell_type":"code","metadata":{"id":"GRwg9QEq6gjm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626290888731,"user_tz":-120,"elapsed":4389,"user":{"displayName":"Giorgio Ottolina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiHqc78Tp0LGpgqAFdZiknA8pFcmI4JwnryQM6I=s64","userId":"08747627632687137907"}},"outputId":"60868ec6-e200-4d35-82cd-9a09fe8cd6ce"},"source":["import os\n","\n","# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n","\n","output_dir = 'stockholm/mlm/mohx_xlm_finetuned/'\n","# output_dir = './content/xlm-roberta_model_save/'\n","\n","# Create output directory if needed\n","if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)\n","\n","print(\"Saving model to %s\" % output_dir)\n","\n","# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n","# They can then be reloaded using `from_pretrained()`\n","model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n","model_to_save.save_pretrained(output_dir)\n","tokenizer.save_pretrained(output_dir)\n","\n","# Good practice: save your training arguments together with the trained model\n","# torch.save(args, os.path.join(output_dir, 'training_args.bin'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Saving model to stockholm/mlm/mohx_xlm_finetuned/\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["('stockholm/mlm/mohx_xlm_finetuned/tokenizer_config.json',\n"," 'stockholm/mlm/mohx_xlm_finetuned/special_tokens_map.json',\n"," 'stockholm/mlm/mohx_xlm_finetuned/sentencepiece.bpe.model',\n"," 'stockholm/mlm/mohx_xlm_finetuned/added_tokens.json')"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"markdown","metadata":{"id":"ySRx1mgTJ7FP"},"source":["##Import Model"]},{"cell_type":"code","metadata":{"id":"YClweiGH6gnE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626290941090,"user_tz":-120,"elapsed":2972,"user":{"displayName":"Giorgio Ottolina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiHqc78Tp0LGpgqAFdZiknA8pFcmI4JwnryQM6I=s64","userId":"08747627632687137907"}},"outputId":"6c738478-2c28-4beb-8504-ea149bcf5e4f"},"source":["!pip install transformers\n","\n","from transformers import XLMRobertaForSequenceClassification\n","\n","output_dir = 'stockholm/mlm/mohx_xlm_finetuned/'\n","\n","print(output_dir)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.8.2)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","stockholm/mlm/mohx_xlm_finetuned/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RHfURAyH6gqX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626290948665,"user_tz":-120,"elapsed":4364,"user":{"displayName":"Giorgio Ottolina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiHqc78Tp0LGpgqAFdZiknA8pFcmI4JwnryQM6I=s64","userId":"08747627632687137907"}},"outputId":"cc4f9503-4692-4869-d23c-8d021e3038af"},"source":["from transformers import XLMRobertaTokenizer\n","import torch\n","# Load the BERT tokenizer.\n","print('Loading XLMRobertaTokenizer...')\n","xlmroberta_tokenizer = XLMRobertaTokenizer.from_pretrained(output_dir)\n","xlmroberta_model = XLMRobertaForSequenceClassification.from_pretrained(output_dir)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading XLMRobertaTokenizer...\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at stockholm/mlm/mohx_xlm_finetuned/ were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at stockholm/mlm/mohx_xlm_finetuned/ and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"UzrXK6QAJ_1U"},"source":["##Masked Language Modeling Pipeline"]},{"cell_type":"code","metadata":{"id":"1Og8o_HG_Mf4"},"source":["from transformers import BertTokenizer, BertForMaskedLM\n","bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","bert_model = BertForMaskedLM.from_pretrained('bert-base-uncased').eval()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G1VoJnSrDpHR"},"source":["from transformers import XLNetTokenizer, XLNetLMHeadModel\n","xlnet_tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n","xlnet_model = XLNetLMHeadModel.from_pretrained('xlnet-base-cased').eval()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JI3V3_oy_Mlp"},"source":["from transformers import XLMRobertaTokenizer, XLMRobertaForMaskedLM\n","xlmroberta_tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n","xlmroberta_model = XLMRobertaForMaskedLM.from_pretrained('xlm-roberta-base').eval()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sT3SnDiB_MoJ"},"source":["from transformers import BartTokenizer, BartForConditionalGeneration\n","bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n","bart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large').eval()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BQGRN8y2_Mq1"},"source":["from transformers import ElectraTokenizer, ElectraForMaskedLM\n","electra_tokenizer = ElectraTokenizer.from_pretrained('google/electra-small-generator')\n","electra_model = ElectraForMaskedLM.from_pretrained('google/electra-small-generator').eval()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jdw6bWex_MuB"},"source":["from transformers import RobertaTokenizer, RobertaForMaskedLM\n","roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n","roberta_model = RobertaForMaskedLM.from_pretrained('roberta-base').eval()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9uz-SE4L_MxE"},"source":["top_k = 10"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zvi2HDlx_M0k"},"source":["def decode(tokenizer, pred_idx, top_clean):\n","    ignore_tokens = string.punctuation + '[PAD]'\n","    tokens = []\n","    for w in pred_idx:\n","        token = ''.join(tokenizer.decode(w).split())\n","        if token not in ignore_tokens:\n","            tokens.append(token.replace('##', ''))\n","    return '\\n'.join(tokens[:top_clean])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SzNySCc_EdM5"},"source":["def encode(tokenizer, text_sentence, add_special_tokens=True):\n","    text_sentence = text_sentence.replace('<mask>', tokenizer.mask_token)\n","    # if <mask> is the last token, append a \".\" so that models dont predict punctuation.\n","    if tokenizer.mask_token == text_sentence.split()[-1]:\n","        text_sentence += ' .'\n","\n","    input_ids = torch.tensor([tokenizer.encode(text_sentence, add_special_tokens=add_special_tokens)])\n","    mask_idx = torch.where(input_ids == tokenizer.mask_token_id)[1].tolist()[0]\n","    return input_ids, mask_idx"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_6rCohYzEdQH"},"source":["def get_all_predictions(text_sentence, top_clean=5):\n","    # ========================= BERT =================================\n","    print(text_sentence)\n","    input_ids, mask_idx = encode(bert_tokenizer, text_sentence)\n","    with torch.no_grad():\n","        predict = bert_model(input_ids)[0]\n","    bert = decode(bert_tokenizer, predict[0, mask_idx, :].topk(top_k).indices.tolist(), top_clean)\n","\n","    # ========================= XLNET LARGE =================================\n","    input_ids, mask_idx = encode(xlnet_tokenizer, text_sentence, False)\n","    perm_mask = torch.zeros((1, input_ids.shape[1], input_ids.shape[1]), dtype=torch.float)\n","    perm_mask[:, :, mask_idx] = 1.0  # Previous tokens don't see last token\n","    target_mapping = torch.zeros((1, 1, input_ids.shape[1]), dtype=torch.float)  # Shape [1, 1, seq_length] => let's predict one token\n","    target_mapping[0, 0, mask_idx] = 1.0  # Our first (and only) prediction will be the last token of the sequence (the masked token)\n","\n","    with torch.no_grad():\n","        predict = xlnet_model(input_ids, perm_mask=perm_mask, target_mapping=target_mapping)[0]\n","    xlnet = decode(xlnet_tokenizer, predict[0, 0, :].topk(top_k).indices.tolist(), top_clean)\n","\n","    # ========================= XLM ROBERTA BASE =================================\n","    input_ids, mask_idx = encode(xlmroberta_tokenizer, text_sentence, add_special_tokens=True)\n","    with torch.no_grad():\n","        predict = xlmroberta_model(input_ids)[0]\n","    xlm = decode(xlmroberta_tokenizer, predict[0, mask_idx, :].topk(top_k).indices.tolist(), top_clean)\n","\n","    # ========================= BART =================================\n","    input_ids, mask_idx = encode(bart_tokenizer, text_sentence, add_special_tokens=True)\n","    with torch.no_grad():\n","        predict = bart_model(input_ids)[0]\n","    bart = decode(bart_tokenizer, predict[0, mask_idx, :].topk(top_k).indices.tolist(), top_clean)\n","\n","    # ========================= ELECTRA =================================\n","    input_ids, mask_idx = encode(electra_tokenizer, text_sentence, add_special_tokens=True)\n","    with torch.no_grad():\n","        predict = electra_model(input_ids)[0]\n","    electra = decode(electra_tokenizer, predict[0, mask_idx, :].topk(top_k).indices.tolist(), top_clean)\n","\n","    # ========================= ROBERTA =================================\n","    input_ids, mask_idx = encode(roberta_tokenizer, text_sentence, add_special_tokens=True)\n","    with torch.no_grad():\n","        predict = roberta_model(input_ids)[0]\n","    roberta = decode(roberta_tokenizer, predict[0, mask_idx, :].topk(top_k).indices.tolist(), top_clean)\n","\n","    return {'bert': bert,\n","            'xlnet': xlnet,\n","            'xlm': xlm,\n","            'bart': bart,\n","            'electra': electra,\n","            'roberta': roberta}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uWusI7WPFAMh"},"source":["text_sentence = 'The stars <mask> towards each other'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RUYcZUE8EdS5"},"source":["get_all_predictions(text_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1eoxLFR1PP39"},"source":["text_sentence = 'The <mask> gravitate towards each other'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wBuSmyu5PTBM"},"source":["get_all_predictions(text_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5v3yBWBBIjqk"},"source":["text_sentence = 'A hot <mask> will revive me'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A9yj0axiIrvK"},"source":["get_all_predictions(text_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rftNaL5SXPsK"},"source":["text_sentence = 'A hot soup will <mask> me'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IwP40s4lXQRf"},"source":["get_all_predictions(text_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1yYrHWT4Xs9B"},"source":["text_sentence = 'They <mask> their way to the top of the mountain .'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_NbK-B4tXtE-"},"source":["get_all_predictions(text_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hth7US3zX6nO"},"source":["text_sentence = 'They clawed their <mask> to the top of the mountain .'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UUeEuSuAX6u5"},"source":["get_all_predictions(text_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XBxERXAfkpl1"},"source":["text_sentence = 'He revived this <mask> of opera .'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F3kRWpgtkprJ"},"source":["get_all_predictions(text_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zYgAUDzLlL0n"},"source":["text_sentence = 'He <mask> this style of opera .'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KDCssIyulL3S"},"source":["get_all_predictions(text_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y4KzKtUnlMDp"},"source":["text_sentence = 'They clawed their <mask> to the top of the mountain .'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZzDP52K0lMGE"},"source":["get_all_predictions(text_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7kHkl0eYlMIx"},"source":["text_sentence = 'They <mask> their way to the top of the mountain .'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cAgkBPvzlMLd"},"source":["get_all_predictions(text_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PvgdVYRplMOT"},"source":["text_sentence = 'We rotate the <mask> so as to maximize the use of the soil .'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rYcOgzO5lMQ6"},"source":["get_all_predictions(text_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MjQX-ZHFpY41"},"source":["text_sentence = 'We <mask> the crops so as to maximize the use of the soil .'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6LxwUxgFpY7x"},"source":["get_all_predictions(text_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LBLcyrIRrN4K"},"source":["text_sentence = \"The <mask> don't harmonize .\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0naDsNGArOJ2"},"source":["get_all_predictions(text_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"geyiwwtsraSz"},"source":["text_sentence = \"The colors don't <mask> .\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ixcb2vWiraU9"},"source":["get_all_predictions(text_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rI79LBuKraX8"},"source":["text_sentence = \"The <mask> is being clogged by these operations .\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fDPELqburaaa"},"source":["get_all_predictions(text_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z-Hdl5Yzracr"},"source":["text_sentence = \"The market is being <mask> by these operations .\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wjT2m_ZKrae_"},"source":["get_all_predictions(text_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H8U6dmh5rahP"},"source":["text_sentence = \"The <mask> was swelling with importance when she spoke of her son .\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jsmQANMMrakG"},"source":["get_all_predictions(text_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JwyeMq4WramX"},"source":["text_sentence = \"The mother was <mask> with importance when she spoke of her son .\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bm6rJNaLrapQ"},"source":["get_all_predictions(text_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gR_gXcdArasa"},"source":["text_sentence = \"Their <mask> inclines us to believe them .\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"451LMuzEraus"},"source":["get_all_predictions(text_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qKZxfiiGraww"},"source":["text_sentence = \"Their language <mask> us to believe them .\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZDWeyiF6uAMr"},"source":["get_all_predictions(text_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IIT_oreAuAPJ"},"source":["text_sentence = \"capture the <mask> of Spring   .\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K3SYIiiruARr"},"source":["get_all_predictions(text_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dOzEmF5TuAUR"},"source":["text_sentence = \"<mask> the essence of Spring   .\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hh5lvO9auAYf"},"source":["get_all_predictions(text_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xdnfYVgyuAau"},"source":["text_sentence = \"He showered her with <mask> .\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vk3Sy8M7uAc1"},"source":["get_all_predictions(text_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vNWagdcGw-ge"},"source":["text_sentence = \"He <mask> her with presents .\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yhsd6XHYw-jS"},"source":["get_all_predictions(text_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-k8XvilYzApT"},"source":["text_sentence = \"Am I supposed to swallow that <mask> ?\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DuqV_OTyzArw"},"source":["get_all_predictions(text_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sX1OQK9QzAuX"},"source":["text_sentence = \"Am I supposed to <mask> that story ?\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pyKAP2WbzAw2"},"source":["get_all_predictions(text_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3yCKJ8m9zAzO"},"source":["text_sentence = \"The <mask> glared down on us .\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FKPC4etfzA1n"},"source":["get_all_predictions(text_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MUVx7iCVzA4c"},"source":["text_sentence = \"The sun <mask> down on us .\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iSBSVhVazA7g"},"source":["get_all_predictions(text_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RN-r20ypz7Xa"},"source":["text_sentence = \"<mask> erupted in the country .\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MLfSfzK3z7aQ"},"source":["get_all_predictions(text_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VmNnImdiz7cn"},"source":["text_sentence = \"Unrest <mask> in the country .\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3tAYJ_LYz7fC"},"source":["get_all_predictions(text_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mlj1TwLnz7hc"},"source":["text_sentence = \"poison someone 's <mask> .\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5c93T2G3z7j2"},"source":["get_all_predictions(text_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DRF-6wPdz7mR"},"source":["text_sentence = \"<mask> someone 's mind .\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RK42ZXWuz7oo"},"source":["get_all_predictions(text_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_pEgM6mo1Mhi"},"source":["text_sentence = \"<mask> were climbing after prices were lowered .\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nKYUjejh1Mkk"},"source":["get_all_predictions(text_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GRhY2L8E1MpY"},"source":["text_sentence = \"Sales were <mask> after prices were lowered .\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1hsn6ogb1MsY"},"source":["get_all_predictions(text_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kDPRHLzk1MvH"},"source":["text_sentence = \"This behavior will ruin your <mask> of winning the election .\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bebXMwBk1MyE"},"source":["get_all_predictions(text_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nQpnz_OZ1M1E"},"source":["text_sentence = \"This behavior will <mask> your chances of winning the election .\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S52Sj9qY4Kdf"},"source":["get_all_predictions(text_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p6G-NKPg4Kgi"},"source":["text_sentence = \"The sales tax is <mask> into the state income tax .\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AWl488T24KjZ"},"source":["get_all_predictions(text_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5Wn31LCL4KnM"},"source":["text_sentence = \"The sales <mask> is absorbed into the state income tax .\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9DxwfRP84Kq6"},"source":["get_all_predictions(text_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t0YtFj2P4Kti"},"source":["text_sentence = \"The invaders spread their <mask> all over the country .\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-hUnPUZ05RLg"},"source":["get_all_predictions(text_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"drBoQVRw5ROX"},"source":["text_sentence = \"The invaders <mask> their language all over the country .\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8gyKXXP-5RRN"},"source":["get_all_predictions(text_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HT2miOG95RTy"},"source":["text_sentence = \"Interns have to <mask> for a few months .\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bNHlGDkb5RXL"},"source":["get_all_predictions(text_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Ia7QR4h7K2O"},"source":["text_sentence = \"They <mask> through the job candidates .\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8MjqRrPo7K5W"},"source":["get_all_predictions(text_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HhozmDYF7yKf"},"source":["text_sentence = \"The dress <mask> her beautiful figure .\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tLaSaEVB7yNd"},"source":["get_all_predictions(text_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6UTT3l5O8X24"},"source":["text_sentence = \"<mask> the child with pride .\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uXi_gyS18YCC"},"source":["get_all_predictions(text_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"febU_uFd8YE7"},"source":["text_sentence = \"The listeners <mask> when he discussed his strange ideas .\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Woc3JPuo8YIB"},"source":["get_all_predictions(text_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8IJQMHgq8YLF"},"source":["text_sentence = \"The path <mask> all the way to the top of the hill .\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZRRV7I6N8YOC"},"source":["get_all_predictions(text_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O78Yp4F7-Zvg"},"source":["text_sentence = \"<mask> the contents of a book into a summary .\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VXsq55KH-Zy4"},"source":["get_all_predictions(text_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pj1o9JckZlJI"},"source":["##TroFi"]},{"cell_type":"code","metadata":{"id":"OIXsPMgqZfxU"},"source":["text_sentence = \"Unfortunately , they wo n't work in the harbor tunnel where business travelers sometimes get <mask> on the way in from the airport . . . \""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vqHT2jWgZf0U"},"source":["get_all_predictions(text_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JsB_vCaxZf3a"},"source":["text_sentence = \"It also vindicates Pratt 's efforts to <mask> problems in customer service that contributed to its loss of dominance in the commercial jet engine field to General Electric Co\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kT_Eh1uxZf6x"},"source":["get_all_predictions(text_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NUbNHCaEZgA0"},"source":["text_sentence = \"Weeks later , '' Penthouse finally realized that Dominion was intentionally <mask> the deal , '' sought other financing , failed , and had to abandon the project , the judge said \""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kcOYQ9WFZgD5"},"source":["get_all_predictions(text_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i07Nf_7PZgHF"},"source":["text_sentence = \"After years of being out of work and on welfare , many of the unemployed now lack the ability or initiative to <mask> openings \""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kup8PuVcZgKD"},"source":["get_all_predictions(text_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-jWvruwdZgNC"},"source":["text_sentence = \"She 's been able to <mask> to this policy , largely because of the nature of her music \""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N4IuwZIOZgQW"},"source":["get_all_predictions(text_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Od0rR-hFZgUX"},"source":["text_sentence = \"The Soviet Union is unwilling and unlikely to <mask> the deadline for withdrawing all its troops from Afghanistan , despite its latest blustering and military maneuvering there , Western and Soviet officials indicated \""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"glsq8_scZgXX"},"source":["get_all_predictions(text_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kh8yk1tvZgaS"},"source":["text_sentence = \"'' I found myself <mask> out on the steps in boot camp , '' he recalls \""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ruORriTxZgdL"},"source":["get_all_predictions(text_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nij2wbi5gNAy"},"source":["text_sentence = \"Last month , Thiokol said it settled the last suit with the estates of the seven crew members <mask> in the Challenger \""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3t04iLXDgNDw"},"source":["get_all_predictions(text_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0JFKFG7dgNGo"},"source":["text_sentence = \"Incumbent Spyros Kyprianou was <mask> out in voting last week\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wBhwAtf7gNJe"},"source":["get_all_predictions(text_sentence)"],"execution_count":null,"outputs":[]}]}